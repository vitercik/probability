---
title: Transformers
---

May 29
: **Transformers overview**
: [Lecture notes](https://vitercik.github.io/ml4do/assets/notes/lecture16.pdf)
: *Supplemental reading:*
: - Chapter 12 of Bishop's "Deep learning: foundations and concepts" [[link 1]](https://searchworks.stanford.edu/view/in00000073280) [[link 2]](https://issuu.com/cmb321/docs/deep_learning_ebook)

June 3
: **Beyond discrete optimization: Transformers as algorithms**
: [Lecture notes](https://vitercik.github.io/ml4do/assets/notes/lecture17.pdf)
: *Supplemental reading:*
: - Garg, Shivam, et al. "What can transformers learn in-context? A case study of simple function classes." NeurIPS'22. [[link]](https://arxiv.org/pdf/2208.01066.pdf)
: - Aky√ºrek, Ekin, et al. "What learning algorithm is in-context learning? Investigations with linear models." ICLR'23. [[link]](https://arxiv.org/pdf/2211.15661)
: - Bai, Yu, et al. "Transformers as statisticians: Provable in-context learning with in-context algorithm selection." NeurIPS'23. [[link]](https://arxiv.org/pdf/2306.04637)
